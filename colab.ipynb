{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer_banco_chatbot.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["PVLwrxpoEEEr","aDnVraTOEIsL","8tu19Qn4Ed_p","9NLLvcFsDNRJ","l3mdHLSfD-jr"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"PVLwrxpoEEEr","colab_type":"text"},"cell_type":"markdown","source":["## Install dependencies"]},{"metadata":{"id":"3Xct6QwTCun4","colab_type":"code","outputId":"05fa2d68-370d-40cb-e555-3b9b1622a115","executionInfo":{"status":"ok","timestamp":1556332541083,"user_tz":240,"elapsed":58033,"user":{"displayName":"Gustavo Arango Argoty","photoUrl":"","userId":"02038744119435648834"}},"colab":{"base_uri":"https://localhost:8080/","height":717}},"cell_type":"code","source":["!pip install tf-nightly-gpu-2.0-preview==2.0.0.dev20190413\n","\n","import tensorflow as tf\n","print(tf.__version__)\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting tf-nightly-gpu-2.0-preview==2.0.0.dev20190413\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/65/2e0abcabd4da641096dad92b4f2ab9d27de508d66efe5c4742ffb6ae4744/tf_nightly_gpu_2.0_preview-2.0.0.dev20190413-cp36-cp36m-manylinux1_x86_64.whl (345.3MB)\n","\u001b[K    100% |████████████████████████████████| 345.3MB 46kB/s \n","\u001b[?25hCollecting google-pasta>=0.1.2 (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190413)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/bb/f1bbc131d6294baa6085a222d29abadd012696b73dcbf8cf1bf56b9f082a/google_pasta-0.1.5-py3-none-any.whl (51kB)\n","\u001b[K    100% |████████████████████████████████| 61kB 26.6MB/s \n","\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190413) (1.16.3)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190413) (0.7.1)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190413) (0.2.2)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190413) (0.33.1)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190413) (1.0.7)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190413) (1.12.0)\n","Collecting tensorflow-estimator-2.0-preview (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190413)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/47/1a7a31baa3e34b33bc241014a6295588019e2922e3546a891e65f7671b8d/tensorflow_estimator_2.0_preview-1.14.0.dev2019042600-py2.py3-none-any.whl (421kB)\n","\u001b[K    100% |████████████████████████████████| 430kB 11.2MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190413) (3.7.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190413) (1.15.0)\n","Collecting tb-nightly<1.15.0a0,>=1.14.0a0 (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190413)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/bd/4c0c1fd31e8d7a6db4b13c7ec3dc7293c4319b134168951a9c9d3f715450/tb_nightly-1.14.0a20190426-py3-none-any.whl (3.1MB)\n","\u001b[K    100% |████████████████████████████████| 3.1MB 7.4MB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190413) (1.0.9)\n","Collecting wrapt>=1.11.1 (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190413)\n","  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190413) (0.7.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview==2.0.0.dev20190413) (1.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-gpu-2.0-preview==2.0.0.dev20190413) (2.8.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-gpu-2.0-preview==2.0.0.dev20190413) (40.9.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview==2.0.0.dev20190413) (0.15.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview==2.0.0.dev20190413) (3.1)\n","Building wheels for collected packages: wrapt\n","  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n","Successfully built wrapt\n","\u001b[31mthinc 6.12.1 has requirement wrapt<1.11.0,>=1.10.0, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n","Installing collected packages: google-pasta, tensorflow-estimator-2.0-preview, tb-nightly, wrapt, tf-nightly-gpu-2.0-preview\n","  Found existing installation: wrapt 1.10.11\n","    Uninstalling wrapt-1.10.11:\n","      Successfully uninstalled wrapt-1.10.11\n","Successfully installed google-pasta-0.1.5 tb-nightly-1.14.0a20190426 tensorflow-estimator-2.0-preview-1.14.0.dev2019042600 tf-nightly-gpu-2.0-preview-2.0.0.dev20190413 wrapt-1.11.1\n","2.0.0-dev20190413\n"],"name":"stdout"}]},{"metadata":{"id":"aDnVraTOEIsL","colab_type":"text"},"cell_type":"markdown","source":["## Integrate Google Drive"]},{"metadata":{"id":"RSdXWxcYZ8aK","colab_type":"code","outputId":"80608e9e-60b6-4fed-fc7d-e29852ae741e","executionInfo":{"status":"ok","timestamp":1556332618717,"user_tz":240,"elapsed":90053,"user":{"displayName":"Gustavo Arango Argoty","photoUrl":"","userId":"02038744119435648834"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"8tu19Qn4Ed_p","colab_type":"text"},"cell_type":"markdown","source":["## Update Git Repository "]},{"metadata":{"id":"rdETFOFoCulD","colab_type":"code","outputId":"905dc5ce-bd27-46a8-91df-9a2928c5d681","executionInfo":{"status":"ok","timestamp":1556248879531,"user_tz":240,"elapsed":13455,"user":{"displayName":"Gustavo Arango Argoty","photoUrl":"","userId":"02038744119435648834"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["%cd /gdrive/Team\\ Drives/umayux/Research/NLP/chatbot/transformer/\n","!git pull\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/gdrive/Team Drives/umayux/Research/NLP/chatbot/transformer\n","Already up to date.\n"],"name":"stdout"}]},{"metadata":{"id":"9NLLvcFsDNRJ","colab_type":"text"},"cell_type":"markdown","source":["## Train the model"]},{"metadata":{"id":"hUdKK461N0u0","colab_type":"code","outputId":"2f886e77-3eb8-45b7-f01f-f518d8ffd291","executionInfo":{"status":"error","timestamp":1555779048962,"user_tz":240,"elapsed":8419,"user":{"displayName":"Gustavo Arango","photoUrl":"https://lh4.googleusercontent.com/-Uu1M-nqufbs/AAAAAAAAAAI/AAAAAAAAAeY/-TMCi_1ZbHg/s64/photo.jpg","userId":"06422916416085568647"}},"colab":{"base_uri":"https://localhost:8080/","height":1663}},"cell_type":"code","source":["import tensorflow_datasets as tfds\n","import tensorflow as tf\n","\n","from src.optimizer import CustomSchedule, loss_function\n","from src.dataset import Dataset\n","from src.model import Transformer\n","import time\n","from src.masking import create_masks\n","import pickle\n","\n","\n","MAX_LENGTH=40\n","BUFFER_SIZE=20000\n","BATCH_SIZE=64\n","EPOCHS=100\n","num_heads=8\n","num_layers=4\n","d_model=128\n","dff=512\n","dropout_rate=0.1\n","test_partition=0.2\n","dataset_file=\"./data/banco/bancobot.tsv\"\n","checkpoint_path=\"./data/banco/\"\n","retrain=True\n","\n","# Build the dataset for training validation\n","dataset = Dataset(filename=dataset_file)\n","dataset.build_train_test(test=test_partition)\n","train_examples, val_examples = dataset.format_train_test()\n","\n","if retrain:\n","    \n","    # loading tokenizers for future predictions\n","    with open(checkpoint_path + \"/tokenizer_source.pickle\", \"rb\") as handle:\n","        tokenizer_source = pickle.load(handle)\n","\n","    with open(checkpoint_path + \"/tokenizer_target.pickle\", \"rb\") as handle:\n","        tokenizer_target = pickle.load(handle)\n","    \n","    # update dataset class with previous data\n","    dataset.tokenizer_source = tokenizer_source\n","    dataset.tokenizer_target = tokenizer_target\n","    \n","else:        \n","    tokenizer_source, tokenizer_target = dataset.tokenizer(train_examples)\n","\n","\n","train_dataset = train_examples.map(dataset.tf_encode)\n","train_dataset = train_dataset.filter(dataset.filter_max_length)\n","train_dataset = train_dataset.cache()\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(\n","    BATCH_SIZE, padded_shapes=([-1], [-1])\n",")\n","train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n","\n","val_dataset = val_examples.map(dataset.tf_encode)\n","val_dataset = val_dataset.filter(dataset.filter_max_length).padded_batch(\n","    BATCH_SIZE, padded_shapes=([-1], [-1])\n",")\n","\n","input_vocab_size = tokenizer_source.vocab_size + 2\n","target_vocab_size = tokenizer_target.vocab_size + 2\n","\n","# Setup the learning rate and optimizer\n","learning_rate = CustomSchedule(d_model)\n","optimizer = tf.keras.optimizers.Adam(\n","    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",")\n","\n","train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n","    name=\"train_accuracy\"\n",")\n","\n","# setup Transformer Model\n","transformer = Transformer(\n","    num_layers,\n","    d_model,\n","    num_heads,\n","    dff,\n","    input_vocab_size,\n","    target_vocab_size,\n","    dropout_rate,\n",")\n","\n","# setup checkpoints\n","ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","# if a checkpoint exists, restore the latest checkpoint.\n","if ckpt_manager.latest_checkpoint:\n","    ckpt.restore(ckpt_manager.latest_checkpoint)\n","    print(\"Latest checkpoint restored!!\")\n","else:\n","    print(\"Initializing from scratch.\")\n","\n","# saving tokenizers\n","with open(checkpoint_path + \"/tokenizer_source.pickle\", \"wb\") as handle:\n","    pickle.dump(tokenizer_source, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","with open(checkpoint_path + \"/tokenizer_target.pickle\", \"wb\") as handle:\n","    pickle.dump(tokenizer_target, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# define training function step\n","@tf.function\n","def train_step(inp, tar):\n","    tar_inp = tar[:, :-1]\n","    tar_real = tar[:, 1:]\n","\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n","        inp, tar_inp\n","    )\n","\n","    with tf.GradientTape() as tape:\n","        predictions, _ = transformer(\n","            inp,\n","            tar_inp,\n","            True,\n","            enc_padding_mask,\n","            combined_mask,\n","            dec_padding_mask,\n","        )\n","        loss = loss_function(tar_real, predictions)\n","\n","    gradients = tape.gradient(loss, transformer.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","\n","    train_loss(loss)\n","    train_accuracy(tar_real, predictions)\n","\n","# training loop\n","for epoch in range(EPOCHS):\n","    start = time.time()\n","\n","    train_loss.reset_states()\n","    train_accuracy.reset_states()\n","\n","    # inp -> portuguese, tar -> english\n","    for (batch, (inp, tar)) in enumerate(train_dataset):\n","        train_step(inp, tar)\n","        if batch % 500 == 0:\n","            print(\n","                \"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n","                    epoch + 1,\n","                    batch,\n","                    train_loss.result(),\n","                    train_accuracy.result(),\n","                )\n","            )\n","\n","    if (epoch + 1) % 5 == 0:\n","        ckpt_save_path = ckpt_manager.save()\n","        print(\n","            \"Saving checkpoint for epoch {} at {}\".format(\n","                epoch + 1, ckpt_save_path\n","            )\n","        )\n","\n","    print(\n","        \"Epoch {} Loss {:.4f} Accuracy {:.4f}\".format(\n","            epoch + 1, train_loss.result(), train_accuracy.result()\n","        )\n","    )\n","\n","    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/readers.py:81: UserWarning: Creating resources inside a function passed to Dataset.flat_map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n","  return filenames.flat_map(read_one_file)\n","/gdrive/Team Drives/umayux/Research/NLP/chatbot/transformer/src/dataset.py:77: UserWarning: Creating resources inside a function passed to Dataset.flat_map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n","  lambda filename: (tf.data.TextLineDataset(filename))\n","/gdrive/Team Drives/umayux/Research/NLP/chatbot/transformer/src/dataset.py:83: UserWarning: Creating resources inside a function passed to Dataset.flat_map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n","  lambda filename: (tf.data.TextLineDataset(filename))\n"],"name":"stderr"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-f2f4b3e39e85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0minput_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtarget_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mdropout_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m )\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/gdrive/Team Drives/umayux/Research/NLP/chatbot/transformer/src/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, rate)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         self.encoder = Encoder(\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/gdrive/Team Drives/umayux/Research/NLP/chatbot/transformer/src/layers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_layers, d_model, num_heads, dff, input_vocab_size, rate)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         self.enc_layers = [\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mEncoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         ]\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1930\u001b[0m     \u001b[0;31m# Append value to self._layers if relevant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m     if (isinstance(value, Layer) or\n\u001b[0;32m-> 1932\u001b[0;31m         trackable_layer_utils.has_weights(value)):\n\u001b[0m\u001b[1;32m   1933\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_create_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_layers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1934\u001b[0m       \u001b[0;31m# We need to check object identity to avoid de-duplicating empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/layer_utils.py\u001b[0m in \u001b[0;36mhas_weights\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;34m\"\"\"Implicit check for Layer-like objects.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;31m# TODO(b/110718070): Replace with isinstance(obj, base_layer.Layer).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m   return (hasattr(obj, \"trainable_weights\")\n\u001b[0m\u001b[1;32m     37\u001b[0m           \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"non_trainable_weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m           and not isinstance(obj, type))\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/data_structures.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0msub_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         extra_variables=self._self_extra_variables)\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/layer_utils.py\u001b[0m in \u001b[0;36mgather_trainable_weights\u001b[0;34m(trainable, sub_layers, extra_variables)\u001b[0m\n\u001b[1;32m     75\u001b[0m   \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msub_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m   trainable_extra_variables = [\n\u001b[1;32m     79\u001b[0m       v for v in extra_variables if v.trainable]\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m       \u001b[0mnested\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_children_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainable_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnested\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_gather_children_attribute\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m   1974\u001b[0m       return list(\n\u001b[1;32m   1975\u001b[0m           itertools.chain.from_iterable(\n\u001b[0;32m-> 1976\u001b[0;31m               getattr(layer, attribute) for layer in nested_layers))\n\u001b[0m\u001b[1;32m   1977\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1974\u001b[0m       return list(\n\u001b[1;32m   1975\u001b[0m           itertools.chain.from_iterable(\n\u001b[0;32m-> 1976\u001b[0;31m               getattr(layer, attribute) for layer in nested_layers))\n\u001b[0m\u001b[1;32m   1977\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    696\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m     return trackable_layer_utils.gather_trainable_weights(\n\u001b[1;32m    700\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_assert_weights_created\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1762\u001b[0m                        \u001b[0;34m'Weights are created when the Model is first called on '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1763\u001b[0m                        \u001b[0;34m'inputs or `build()` is called with an `input_shape`.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1764\u001b[0;31m                        self.name)\n\u001b[0m\u001b[1;32m   1765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Weights for model sequential have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`."]}]},{"metadata":{"id":"BhnIfK72DS63","colab_type":"text"},"cell_type":"markdown","source":["## Test the model"]},{"metadata":{"id":"4DgF6kv0DWCu","colab_type":"code","outputId":"902f3459-afd5-4545-ef1a-662af9468010","executionInfo":{"status":"ok","timestamp":1556332679934,"user_tz":240,"elapsed":9343,"user":{"displayName":"Gustavo Arango Argoty","photoUrl":"","userId":"02038744119435648834"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["%cd /gdrive/Team\\ Drives/umayux/Research/NLP/chatbot/transformer/\n","\n","import tensorflow_datasets as tfds\n","import tensorflow as tf\n","import utensor.dataset as dt\n","from utensor.optimizer import CustomSchedule, loss_function\n","from utensor.model import Transformer\n","import time\n","from utensor.masking import create_masks\n","import pickle\n","import matplotlib.pyplot as plt\n","\n","\n","\n","checkpoint_path=\"./data/banco/\"\n","d_model = 128\n","MAX_LENGTH=60\n","BUFFER_SIZE=20000\n","BATCH_SIZE=64\n","num_heads=8\n","num_layers=4\n","d_model=128\n","dff=512\n","dropout_rate=0.1\n","\n","\n","\n","def restore():\n","    \n","    # loading tokenizers for future predictions\n","    tokenizer_source = pickle.load(open(checkpoint_path + './tokenizer_source.pickle', 'rb'))\n","    tokenizer_target = pickle.load(open(checkpoint_path + './tokenizer_target.pickle', 'rb'))\n","\n","    input_vocab_size = tokenizer_source.vocab_size + 2\n","    target_vocab_size = tokenizer_target.vocab_size + 2\n","\n","    learning_rate = CustomSchedule(d_model)\n","    optimizer = tf.keras.optimizers.Adam(\n","        learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n","    )\n","\n","    transformer = Transformer(\n","        num_layers,\n","        d_model,\n","        num_heads,\n","        dff,\n","        input_vocab_size,\n","        target_vocab_size,\n","        dropout_rate,\n","    )\n","\n","\n","    ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n","    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n","\n","    # if a checkpoint exists, restore the latest checkpoint.\n","    if ckpt_manager.latest_checkpoint:\n","        ckpt.restore(ckpt_manager.latest_checkpoint)\n","        print(\"Latest checkpoint restored!!\")\n","    else:\n","        print(\"Initializing from scratch.\")\n","        \n","    return transformer, tokenizer_source, tokenizer_target\n","\n","\n","           \n","    \n","def evaluate(inp_sentence):\n","    start_token = [tokenizer_source.vocab_size]\n","    end_token = [tokenizer_source.vocab_size + 1]\n","\n","    # inp sentence is portuguese, hence adding the start and end token\n","    inp_sentence = start_token + tokenizer_source.encode(inp_sentence) + end_token\n","    encoder_input = tf.expand_dims(inp_sentence, 0)\n","\n","    # as the target is english, the first word to the transformer should be the\n","    # english start token.\n","    decoder_input = [tokenizer_target.vocab_size]\n","    output = tf.expand_dims(decoder_input, 0)\n","\n","    for i in range(MAX_LENGTH):\n","        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n","            encoder_input, output)\n","\n","        # predictions.shape == (batch_size, seq_len, vocab_size)\n","        predictions, attention_weights = transformer(encoder_input, \n","                                                     output,\n","                                                     False,\n","                                                     enc_padding_mask,\n","                                                     combined_mask,\n","                                                     dec_padding_mask)\n","\n","        # select the last word from the seq_len dimension\n","        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n","\n","        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","        # return the result if the predicted_id is equal to the end token\n","        if tf.equal(predicted_id, tokenizer_target.vocab_size+1):\n","            return tf.squeeze(output, axis=0), attention_weights\n","\n","        # concatentate the predicted_id to the output which is given to the decoder\n","        # as its input.\n","        output = tf.concat([output, predicted_id], axis=-1)\n","\n","    return tf.squeeze(output, axis=0), attention_weights\n","\n","\n","\n","def translate(sentence):\n","    result, attention_weights = evaluate(sentence)\n","\n","    predicted_sentence = tokenizer_target.decode([i for i in result \n","                                            if i < tokenizer_target.vocab_size])  \n","\n","    print('Pregunta: {}'.format(sentence))\n","    print('Respuesta UmyBot: {}'.format(predicted_sentence))\n","\n","\n","\n","transformer, tokenizer_source, tokenizer_target = restore()\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/gdrive/Team Drives/umayux/Research/NLP/chatbot/transformer\n","Latest checkpoint restored!!\n"],"name":"stdout"}]},{"metadata":{"id":"vDO-wQ-E56qb","colab_type":"code","outputId":"b5d9eec9-22ed-44b6-9e99-a22559aa291f","executionInfo":{"status":"ok","timestamp":1556332690782,"user_tz":240,"elapsed":7998,"user":{"displayName":"Gustavo Arango Argoty","photoUrl":"","userId":"02038744119435648834"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["translate('banco_falabella dicen que no están operativo')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Pregunta: banco_falabella dicen que no están operativo\n","Respuesta UmyBot: hola @fefith, hemos realizado pruebas de transferencia y no presentamos inconvenientes.\n"],"name":"stdout"}]},{"metadata":{"id":"QLpF3esoFT31","colab_type":"code","outputId":"dbac0380-e193-41c6-aa6d-83b2fe582987","executionInfo":{"status":"ok","timestamp":1556254883833,"user_tz":240,"elapsed":2903,"user":{"displayName":"Gustavo Arango Argoty","photoUrl":"","userId":"02038744119435648834"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["translate('@bancolombia #bancoeterno se cagaron literalmente el servicio. ahora son menos filas pero más demoras con el servicio.')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Pregunta: @bancolombia #bancoeterno se cagaron literalmente el servicio. ahora son menos filas pero más demoras con el servicio.\n","Respuesta UmyBot: ¡hola! cuéntanos por favor si haces referencia a un poco más de tu comentario, para poder ayudarte. saludos. ana\n"],"name":"stdout"}]},{"metadata":{"id":"l3mdHLSfD-jr","colab_type":"text"},"cell_type":"markdown","source":["## Predict examples"]},{"metadata":{"id":"a2ebWi0cUzow","colab_type":"code","outputId":"c3e562e0-d5fd-4f58-cfa7-f631383ca55c","executionInfo":{"status":"ok","timestamp":1556252317547,"user_tz":240,"elapsed":1220,"user":{"displayName":"Gustavo Arango Argoty","photoUrl":"","userId":"02038744119435648834"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["%cd /gdrive/Team\\ Drives/umayux/Research/NLP/chatbot/transformer/\n","\n","import tensorflow_datasets as tfds\n","import tensorflow as tf\n","import utensor.dataset as dt\n","from utensor.optimizer import CustomSchedule, loss_function\n","from utensor.model import Transformer\n","import time\n","from utensor.masking import create_masks\n","import pickle\n","import matplotlib.pyplot as plt\n","\n","checkpoint_path=\"./data/banco/\"\n","d_model = 128\n","MAX_LENGTH=40\n","BUFFER_SIZE=20000\n","BATCH_SIZE=64\n","num_heads=8\n","num_layers=4\n","dff=512\n","dropout_rate=0.1\n","\n","def restore():\n","\n","    # loading tokenizers for future predictions\n","    tokenizer_source = pickle.load(open(checkpoint_path+'/tokenizer_source.pickle', 'rb'))\n","    tokenizer_target = pickle.load(open(checkpoint_path+'/tokenizer_target.pickle', 'rb'))\n","\n","    input_vocab_size = tokenizer_source.vocab_size + 2\n","    target_vocab_size = tokenizer_target.vocab_size + 2\n","\n","    learning_rate = CustomSchedule(d_model)\n","    optimizer = tf.keras.optimizers.Adam(\n","        learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n","    )\n","\n","    transformer = Transformer(\n","        num_layers,\n","        d_model,\n","        num_heads,\n","        dff,\n","        input_vocab_size,\n","        target_vocab_size,\n","        dropout_rate,\n","    )\n","\n","    ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n","    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n","\n","    # if a checkpoint exists, restore the latest checkpoint.\n","    if ckpt_manager.latest_checkpoint:\n","        ckpt.restore(ckpt_manager.latest_checkpoint)\n","        print(\"Latest checkpoint restored!!\")\n","    else:\n","        print(\"Initializing from scratch.\")\n","\n","    return transformer, tokenizer_source, tokenizer_target\n","        \n","\n","        \n","def evaluate(inp_sentence):\n","    start_token = [tokenizer_source.vocab_size]\n","    end_token = [tokenizer_source.vocab_size + 1]\n","    \n","    \n","    # inp sentence is portuguese, hence adding the start and end token\n","    inp_sentence = start_token + tokenizer_source.encode(inp_sentence) + end_token\n","    encoder_input = tf.expand_dims(inp_sentence, 0)\n","\n","    # as the target is english, the first word to the transformer should be the\n","    # english start token.\n","    decoder_input = [tokenizer_target.vocab_size]\n","    output = tf.expand_dims(decoder_input, 0)\n","\n","    for i in range(40):\n","        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n","        encoder_input, output)\n","\n","    # predictions.shape == (batch_size, seq_len, vocab_size)\n","    predictions, attention_weights = transformer(encoder_input, \n","                                                 output,\n","                                                 False,\n","                                                 enc_padding_mask,\n","                                                 combined_mask,\n","                                                 dec_padding_mask)\n","\n","    # select the last word from the seq_len dimension\n","    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n","\n","    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","    # return the result if the predicted_id is equal to the end token\n","    if tf.equal(predicted_id, tokenizer_target.vocab_size+1):\n","        return tf.squeeze(output, axis=0), attention_weights\n","\n","    # concatentate the predicted_id to the output which is given to the decoder\n","    # as its input.\n","    output = tf.concat([output, predicted_id], axis=-1)\n","\n","    return tf.squeeze(output, axis=0), attention_weights\n","\n","\n","\n","\n","def translate(sentence, plot=''):\n","    \n","    result, attention_weights = evaluate(sentence)\n","\n","    predicted_sentence = tokenizer_target.decode([i for i in result \n","                                            if i < tokenizer_target.vocab_size])  \n","\n","    print('Pregunta: {}'.format(sentence))\n","    print('Respuesta UmyBot: {}'.format(predicted_sentence))\n","\n","    #   if plot:\n","    #     plot_attention_weights(attention_weights, sentence, result, plot)\n","\n","    \n","    \n","transformer, tokenizer_source, tokenizer_target = restore()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/gdrive/Team Drives/umayux/Research/NLP/chatbot/transformer\n","Latest checkpoint restored!!\n"],"name":"stdout"}]},{"metadata":{"id":"0xqAvczzvMok","colab_type":"code","outputId":"96cc8f2c-4309-44af-950c-768042801af8","executionInfo":{"status":"ok","timestamp":1556252323817,"user_tz":240,"elapsed":1518,"user":{"displayName":"Gustavo Arango Argoty","photoUrl":"","userId":"02038744119435648834"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["sentence = '@bancolombia #bancoeterno se cagaron literalmente el servicio. ahora son menos filas pero más demoras con el servicio.'\n","translate(sentence, plot='')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Pregunta: @bancolombia #bancoeterno se cagaron literalmente el servicio. ahora son menos filas pero más demoras con el servicio.\n","Respuesta UmyBot: ¡\n"],"name":"stdout"}]},{"metadata":{"id":"97w-DBMBn3JE","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","data = pd.read_csv('./data/banco/bancobot.tsv.test', sep='\\t', names=['source', 'target'])\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1DOJWQxVCuLm","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","for ix,i in data.iterrows():\n","    translate(\n","        i['source']\n","    )\n","    print(\"Respuesta Humano: {}\".format(i['target']))\n","    print('\\n\\n')\n","    \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ISiMWAVqft2e","colab_type":"code","colab":{}},"cell_type":"code","source":["def plot_attention_weights(attention, sentence, result, layer, tokenizer_source, tokenizer_target):\n","  fig = plt.figure(figsize=(30, 38))\n","  \n","  sentence = tokenizer_source.encode(sentence)\n","  \n","  attention = tf.squeeze(attention[layer], axis=0)\n","  \n","  for head in range(attention.shape[0]):\n","    ax = fig.add_subplot(8, 1, head+1)\n","    \n","    # plot the attention weights\n","    ax.matshow(attention[head][:-1, :], cmap='viridis')\n","\n","    fontdict = {'fontsize': 10}\n","    \n","    ax.set_xticks(range(len(sentence)+2))\n","    ax.set_yticks(range(len(result)))\n","    \n","    ax.set_ylim(len(result)-1.5, -0.5)\n","        \n","    ax.set_xticklabels(\n","        ['<start>']+[tokenizer_source.decode([i]) for i in sentence]+['<end>'], \n","        fontdict=fontdict, rotation=90)\n","    \n","    ax.set_yticklabels([tokenizer_target.decode([i]) for i in result \n","                        if i < tokenizer_target.vocab_size], \n","                       fontdict=fontdict)\n","    \n","    ax.set_xlabel('Head {}'.format(head+1))\n","  \n","  plt.tight_layout()\n","  plt.show()"],"execution_count":0,"outputs":[]}]}